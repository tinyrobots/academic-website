+++
title = "Symposium organiser and speaker at TeaP 2021"

# Schedule page publish date (NOT talk date).
publishDate = 2017-01-01T00:00:00

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Alban Flachot", "Katherine R. Storrs", "Christina Funke", "Kshitij Diwedi", "Martin Hebart", "Katharina Dobs"]

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date = 2021-03-15T09:00:00
date_end = 2020-03-15T10:30:00
all_day = false

# Location of event.
location = "TeaP"

# Name of event and optional event URL.
event = "Developments in deep neural network models of perception: From low- to high-level vision"
event_url = "https://cops.ifp.uni-ulm.de/teap2021/"

# Abstract. What's your talk about?
abstract = "Deep neural networks (DNNs) have revolutionised computer vision, often now recognising objects and faces as well as humans can. An initial wave of fMRI and electrophysiological studies around 2015 showed that features in object-recognition-trained DNNs predict neural responses in high-level visual cortex. DNNs have since flourished as models of perception, with diverse custom networks, training tasks, and evaluation methods emerging. The talks in this symposium highlight a range of approaches to current challenges, as well as spanning the gamut of visual processing from color perception, through material and contour perception, to object and face recognition. \n One open challenge is building DNNs with ecologically plausible training tasks and experience. Katherine Storrs explores how perceptual dimensions can form in DNNs through unsupervised statistical learning, without the need for labelled examples. Katharina Dobs and Kshitij Dwivedi tease apart how experience of different visual diets and ecologically-relevant learning objectives affect representations in DNNs, and their performance as models of brain and behaviour. \n As DNNs become more powerful, it becomes crucial to find nuanced ways to compare their perception to ours. Alban Flachot uses a large-scale custom dataset to probe how the fundamental visual competencies of colour perception and constancy develop. Judy Borowski shows how tasks like closed-contour detection present particular challenges to artificial vision, providing leverage to study functional differences. Finally, the talks showcase approaches for peering inside the 'black box' of DNNs. For example, Martin Hebart presents a novel data-driven method for finding interpretable dimensions in DNNs, and compares these to those underlying human perception. Collectively, the talks capture the diversity of DNN modelling in vision science."

# Summary. An optional shortened abstract.
summary = ""

# Is this a featured talk? (true/false)
featured = true

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = ["display"]

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""

# Optional filename of your slides within your talk folder or a URL.
url_slides = ""

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = []

# Links (optional).
url_pdf = ""
url_video = ""
url_code = ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Left"
+++
