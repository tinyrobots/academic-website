+++
title = "Invited speaker at The Skin of Things online art-science conference"

# Schedule page publish date (NOT talk date).
publishDate = 2017-01-01T00:00:00

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = ["Katherine R. Storrs"]

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date = 2021-03-15T15:00:00
date_end = 2020-03-15T15:30:00
all_day = false

# Location of event.
location = "The Skin of Things"

# Name of event and optional event URL.
event = "Learning about material properties by learning about images"
event_url = "https://theskinofthings2021.github.io/"

# Abstract. What's your talk about?
abstract = "A photograph or painting of a glazed vase might consist of irregularly-shaped bright patches, small white dots, and large low-contrast gradients—yet we immediately see these as reflections on the glossy surface, sharp highlights, and the smooth shading of a rounded object. How are we able to disentangle complex interacting factors like 3D shape, lighting and surface reflectance to perceive individual physical quantities, like how glossy a surface is? To make matters more difficult, the brain must solve this problem based on visual experience alone, since we are never told the true reflectance values of surfaces from which we might learn. We approached the question by using a 3D rendering engine to render tens of thousands of images of simple visual scenes (bumpy surfaces with different material properties, seen under diverse lighting conditions), and then training artificial visual systems (deep neural networks) to generate new images that followed the same 'rules.' This is an 'unsupervised learning' goal; the network learns high-level statistical regularities in images, that allow it to create entirely novel images that look like plausible real surfaces, without ever being told about physical properties. We found that, in their internal representations, the unsupervised networks spontaneously clustered images by physical properties like reflectance and illumination, despite receiving no explicit information about them. Most intriguingly, their representations also predict specific patterns of 'successes' and 'errors' in human perception. The unsupervised networks predict human gloss perception better than ground truth, supervised networks, or various baseline models, and can predict, on an image-by-image basis, illusions of gloss perception caused by interactions between material, shape, and lighting. We suggest that the brain learns about materials—and perhaps many other properties of the world!—by learning the ways in which images vary, within and across our moment-to-moment visual experience."

# Summary. An optional shortened abstract.
summary = ""

# Is this a featured talk? (true/false)
featured = true

# Tags (optional).
#   Set `tags = []` for no tags, or use the form `tags = ["A Tag", "Another Tag"]` for one or more tags.
tags = ["display"]

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references 
#   `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides = ""

# Optional filename of your slides within your talk folder or a URL.
url_slides = ""

# Projects (optional).
#   Associate this talk with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["deep-learning"]` references 
#   `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects = []

# Links (optional).
url_pdf = ""
url_video = ""
url_code = ""

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
[image]
  # Caption (optional)
  caption = ""

  # Focal point (optional)
  # Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
  focal_point = "Left"
+++
